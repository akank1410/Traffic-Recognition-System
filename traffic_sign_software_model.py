# -*- coding: utf-8 -*-
"""traffic sign software model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GYb3clx-7EfV-1QuSXA_w_ABUgPVlYn7
"""

import os
os.getcwd()

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)



os.getcwd()

#install Kaggle
!pip install -q kaggle

from google.colab import files
files.upload()

#create a kaggle folder
! mkdir ~/.kaggle

#copy the kaggle.json to folder created
! cp kaggle.json  ~/.kaggle

#Permission for the json to act
! chmod 600  ~/.kaggle

#Permission for the json to act
! chmod 600 '/content/gdrive/MyDrive/traffic sign content/data split/kaggle.json'

"""**Importing the dataset from the kaggle**"""

# 1. GTSRB - German Traffic Sign Recognition Benchmark
!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign

!unzip gtsrb-german-traffic-sign.zip

#copy the kaggle.json to folder created
! cp -r '/content/meta' '/content/gdrive/MyDrive/traffic sign content'

# 2.https://www.kaggle.com/datasets/ahemateja19bec1025/traffic-sign-dataset-classification
!kaggle datasets download -d ahemateja19bec1025/traffic-sign-dataset-classification

!unzip traffic-sign-dataset-classification.zip

"""**Studying the dataset1**


"""

dir = "/content/meta"
os.listdir(dir) #this function will list all the files in the directory mentioned

path = '38.png'
os.path.join(dir, path) #this function joins paths together

"""REFERENCE: https://www.techiedelight.com/list-all-subdirectories-in-directory-python/"""

initial_count_file = 0
initial_count_folder = 0
dir = "/content/meta"
for path in os.listdir(dir):
    if os.path.isfile(os.path.join(dir, path)):
        initial_count_file += 1
    if os.path.isdir(os.path.join(dir, path)):
        initial_count_folder += 1
print("Number of files in Meta Folder of GTSRB ",initial_count_file)
print("Number of sub-folders in  Meta Folder of GTSRB ",initial_count_folder)

initial_count_file = 0
initial_count_folder = 0
dir = "/content/test"
for path in os.listdir(dir):
    if os.path.isfile(os.path.join(dir, path)):
        initial_count_file += 1
    if os.path.isdir(os.path.join(dir, path)):
        initial_count_folder += 1
print("Number of files in Test Folder of GTSRB ",initial_count_file)
print("Number of sub-folders in Test Folder of GTSRB ",initial_count_folder)

initial_count_file = 0
initial_count_folder = 0
dir = "/content/train"
for path in os.listdir(dir):
    if os.path.isfile(os.path.join(dir, path)):
        initial_count_file += 1
    if os.path.isdir(os.path.join(dir, path)):
        initial_count_folder += 1
print("Number of images in Train Folder of GTSRB ",initial_count_file)
print("Number of classes in Train Folder of GTSRB ",initial_count_folder)

class_count={}
dir = "/content/train"
for path in os.listdir(dir):
    initial_count_file = 0
    sub_dir = os.path.join(dir, path)
    for file in os.listdir(sub_dir):
      if os.path.isfile(os.path.join(sub_dir, file)):
        initial_count_file += 1
    class_count[int(path)]=initial_count_file 
    print("Number of files in ",path," class of Train Folder of GTSRB ",initial_count_file)

#Sorting the classes dictionary with respect to keys

sorted(class_count.keys())
class_count

# Creates a sorted dictionary (sorted by key)
from collections import OrderedDict
print(class_count)
class_count = OrderedDict(sorted(class_count.items()))
print(class_count)

class_count.values()

print("No. of classes : ",len(os.listdir('/content/train')))

count=0
for path in os.listdir('/content/train'):
  for file in os.listdir(os.path.join('/content/train',path)):
    count+=1

print("total no. of images",count)

#dictionary to label all traffic signs class.
classes = { 0:'Speed limit (20km/h)',
            1:'Speed limit (30km/h)', 
            2:'Speed limit (50km/h)', 
            3:'Speed limit (60km/h)', 
            4:'Speed limit (70km/h)', 
            5:'Speed limit (80km/h)', 
            6:'End of speed limit (80km/h)', 
            7:'Speed limit (100km/h)', 
            8:'Speed limit (120km/h)', 
            9:'No passing', 
            10:'No passing veh over 3.5 tons', 
            11:'Right-of-way at intersection', 
            12:'Priority road', 
            13:'Give Way', 
            14:'Stop', 
            15:'No vehicles', 
            16:'Veh > 3.5 tons prohibited', 
            17:'No entry', 
            18:'General caution', 
            19:'Dangerous curve left', 
            20:'Dangerous curve right', 
            21:'Double curve', 
            22:'Bumpy road', 
            23:'Slippery road', 
            24:'Road narrows on the right', 
            25:'Road work', 
            26:'Traffic signals', 
            27:'Pedestrians', 
            28:'Children crossing', 
            29:'Bicycles crossing', 
            30:'Beware of ice/snow',
            31:'Wild animals crossing', 
            32:'End speed + passing limits', 
            33:'Turn right ahead', 
            34:'Turn left ahead', 
            35:'Ahead only', 
            36:'Go straight or right', 
            37:'Go straight or left', 
            38:'Keep right', 
            39:'Keep left', 
            40:'Roundabout mandatory', 
            41:'End of no passing', 
            42:'End no passing veh > 3.5 tons' }

# Reference : https://www.adamsmith.haus/python/answers/how-to-display-the-value-of-each-bar-in-a-bar-chart-using-matplotlib-in-python
#Reference : https://www.geeksforgeeks.org/bar-plot-in-matplotlib/

#Figure Size
plt.figure(figsize=(16,16))
#Horizontal Bar Plot
plt.barh(list(classes.values()), list(class_count.values()))
#Add Plot Title
plt.title('Number of Training Images in Each class ', loc ='left',fontsize= 22 )
plt.xlabel('Number of image file')
plt.ylabel('Classes')

# Add annotation to bars
for index,value in enumerate(list(class_count.values())):
    plt.text(value, index, str(value))

m=list(enumerate(list(class_count.values())))
m

#reference : https://www.delftstack.com/howto/matplotlib/how-to-add-title-to-subplots-in-matplotlib/
#If you use Matlab-like style in the interactive plotting, then you could use plt.gca() 
#to get the reference of the current axes of the subplot and combine set_title() or title.set_text() method 
#to set title to the subplots in Matplotlib.

import matplotlib.pyplot as plt
from keras.preprocessing.image import load_img, img_to_array

picture_size = 48
folder_path = '/content/train/'
plt.figure(figsize = (16,18))
i=1
for key in classes:
  plt.subplot(9,5,i)
  img = load_img(folder_path+str(key)+"/"+os.listdir(folder_path+ str(key))[3], target_size=(picture_size, picture_size))
  plt.imshow(img)
  plt.gca().set_title(classes[key])
  i = i+1
plt.tight_layout()
plt.show()

folder_path = "/content/train"
plt.figure(figsize = (12,12))
for path in os.listdir(folder_path):
  sub_class = os.path.join(folder_path,path)
for i in range(2, 27, 1):
  plt.subplot(5,9,i-1)
  img = load_img(folder_path+"meta/"+os.listdir(folder_path+'meta')[i], target_size=(picture_size, picture_size))
  print(img)
  plt.imshow(img)
plt.show()

"""**Studying dataset2**"""

source_folder= '/content/traffic_Data/DATA/36'
destination_folder ='/content/traffic_Data/DATA/30' 

for path in os.listdir(source_folder):
  src_path = os.path.join(source_folder,path)
  dst_path = os.path.join(destination_folder,path)
  shutil.move(src_path,dst_path)

shutil.move('/content/traffic_Data/DATA/036_0018.png', '/content/traffic_Data/DATA/30')

shutil.move(src_path,dst_path)

classes1 = {0:'Speed limit (5km/h)',
            1:'Speed limit (15km/h)',
            2:'Speed limit (30km/h)',
            3:'Speed limit (40km/h)',
            4:'Speed limit (50km/h)',
            5:'Speed limit (60km/h)',
            6:'Speed limit (70km/h)',
            8:'Dont Go straight or left',
            7:'speed limit (80km/h)',
            9:'Dont Go straight or Right',
            10:'Dont Go straight',
            11:'Dont Go Left',
            12:'Dont Go Left or Right',
            13:'Dont Go Right',
            14:'Dont overtake from Left',
            15:'No Uturn',
            16:'No Car',
            17:'No horn',
            18:'End of speed limit (40km/h)',
            19:'End of speed limit (50km/h)',
            20:'Go straight or right',
            21:'Go straight',
            22:'Go Left',
            23:'Go Left or right',
            24:'Go Right',
            25:'keep Left',
            26:'keep Right',
            27:'Roundabout mandatory',
            28:'watch out for cars',
            29:'Horn',
            30:'Bicycles crossing',
            31:'Uturn',
            32:'Road Divider',
            33:'Traffic signals',
            34:'Danger Ahead',
            35:'Zebra Crossing',
            36:'NULL',
            37:'Children crossing',
            38:'Dangerous curve to the left',
            39:'Dangerous curve to the right',
            40:'Unknown1',
            41:'Unknown2',
            42:'Unknown3',
            43:'Go right or straight',
            44:'Go left or straight',
            45:'Unknown4',
            46:'ZigZag Curve',
            47:'Train Crossing',
            48:'Under Construction',
            49:'Unknown5',
            50:'Fences',
            51:'Heavy Vehicle Accidents',
            52:'Unknown6',
            53:'Give Way',
            54:'No stopping',
            55:'No entry',
            56:'Unknown7',
            57:'Unknown8'}

initial_count_file = 0
initial_count_folder = 0
dir = "/content/traffic_Data/TEST"
for path in os.listdir(dir):
    if os.path.isfile(os.path.join(dir, path)):
        initial_count_file += 1
    if os.path.isdir(os.path.join(dir, path)):
        initial_count_folder += 1
print("Number of files in Test Folder of traffic_Data ",initial_count_file)
print("Number of sub-folders in Test Folder of traffic_Data ",initial_count_folder)

class_count1={}
dir = "/content/traffic_Data/DATA"
for path in os.listdir(dir):
    initial_count_file = 0
    sub_dir = os.path.join(dir, path)
    for file in os.listdir(sub_dir):
      if os.path.isfile(os.path.join(sub_dir, file)):
        initial_count_file += 1
    class_count1[int(path)]=initial_count_file 
    print("Number of files in ",path," class of Train Folder of traffic_Data ",initial_count_file)

print("No. of classes : ",len(os.listdir('/content/traffic_Data/DATA')))

count=0
for path in os.listdir('/content/traffic_Data/DATA'):
  for file in os.listdir(os.path.join('/content/traffic_Data/DATA',path)):
    count+=1

print("total no. of images",count)

# Creates a sorted dictionary (sorted by key)
from collections import OrderedDict
print(class_count1)
class_count1 = OrderedDict(sorted(class_count1.items()))
print(class_count1)



# Reference : https://www.adamsmith.haus/python/answers/how-to-display-the-value-of-each-bar-in-a-bar-chart-using-matplotlib-in-python
#Reference : https://www.geeksforgeeks.org/bar-plot-in-matplotlib/

#Figure Size
plt.figure(figsize=(16,18))
#Horizontal Bar Plot
plt.barh(list(classes1.values()), list(class_count1.values()), height=0.8)
#Add Plot Title
plt.title('Number of Training Images in Each class ', loc ='left',fontsize= 22 )
plt.xlabel('Number of image file')
plt.ylabel('Classes')


# Add annotation to bars
for index,value in enumerate(list(class_count1.values())):
    plt.text(value, index, str(value))

#reference : https://www.delftstack.com/howto/matplotlib/how-to-add-title-to-subplots-in-matplotlib/
#If you use Matlab-like style in the interactive plotting, then you could use plt.gca() 
#to get the reference of the current axes of the subplot and combine set_title() or title.set_text() method 
#to set title to the subplots in Matplotlib.

import matplotlib.pyplot as plt
from keras.preprocessing.image import load_img, img_to_array

picture_size = 48
folder_path = '/content/traffic_Data/DATA/'
plt.figure(figsize = (16,18))
i=1
for key in classes1:
  plt.subplot(12,5,i)
  if(key==36):
    continue
  img = load_img(folder_path+str(key)+"/"+os.listdir(folder_path+ str(key))[0], target_size=(picture_size, picture_size))
  plt.imshow(img)
  plt.gca().set_title(classes1[key])
  i = i+1
plt.tight_layout()
plt.show()

m = list(classes1.values()) 
n =list(class_count1.values())

for i in range(len(m)):
  print(i, " : ",m[i]," : ",n[i])

list(enumerate(list(class_count1.values())))

"""**COMBINING THE TWO DATASET**"""

import shutil
#No entry
source_folder= '/content/traffic_Data/DATA/55'
destination_folder ='/content/Train/17' 

for path in os.listdir(source_folder):
  src_path = os.path.join(source_folder,path)
  shutil.copy(src_path,destination_folder)

#give way
import shutil
source_folder= '/content/traffic_Data/DATA/53'
destination_folder ='/content/traffic_Data/DATA/13' 

for path in os.listdir(source_folder):
  src_path = os.path.join(source_folder,path)
  dst_path = os.path.join(destination_folder,path)
  shutil.move(src_path,dst_path)



picture_size = 48
folder_path_ = "/content/"

#/content/sample_data/Traffic Sign dataset/Meta/0.png

plt.figure(figsize = (12,12))
for i in range(1, 26, 1):
  plt.subplot(5,5,i)
  img = load_img(folder_path_+"batch/"+os.listdir(folder_path_+'batch')[i+25], target_size=(picture_size, picture_size))
  plt.imshow(img)
plt.show()

"""**---------------------------------------------------------------------------------------**"""

import matplotlib.image as mpimg
import matplotlib.pyplot as plt

img = mpimg.imread('/content/traffic_Data/DATA/36/036_0018.png')

plt.imshow(img)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import os
import tensorflow as tf
from PIL import Image

#Importing Deep Learining Libraries
from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.image import ImageDataGenerator

"""**DISPLAYING IMAGES**"""

set(range(1, 12, 1))

folder_path = '/content/'
picture_size = 48
plt.figure(figsize = (12,12))
for i in range(1,46, 1):
  plt.subplot(9,5,i)
  if(i==6 or i==29):
    img = load_img('/content/meta/5.png', target_size=(picture_size, picture_size))
  elif i==29:
    img = load_img('/content/meta/28.png', target_size=(picture_size, picture_size))
  else:
    img = load_img(folder_path+"meta/"+os.listdir(folder_path+'meta')[i-1], target_size=(picture_size, picture_size))
  plt.imshow(img)
plt.show()

"""**DATA EXTRACTION**"""

categ = {'1': '30 speed limit', '4':'70 speed limit', '15':'no entry', '18':'go slow', '33':'right hand curve', '34':'left hand curve' }

#reference : https://www.delftstack.com/howto/matplotlib/how-to-add-title-to-subplots-in-matplotlib/
#If you use Matlab-like style in the interactive plotting, then you could use plt.gca() 
#to get the reference of the current axes of the subplot and combine set_title() or title.set_text() method 
#to set title to the subplots in Matplotlib.

plt.figure(figsize = (12,12))
i=1
for key in classes:
  plt.subplot(2,3,i)
  img = load_img(folder_path_n+"train/"+key+"/"+os.listdir(folder_path_n+"train/"+ key)[1], target_size=(picture_size, picture_size))
  plt.imshow(img)
  plt.gca().set_title(categ[key])
  i = i+1

plt.show()

import pandas as pd
import numpy as np

df = pd.read_csv('/content/gdrive/MyDrive/traffic sign content/Meta.csv')

df

df_train = pd.read_csv('/content/sample_data/Traffic Sign dataset/Train.csv')
df_train

df_train.head()

# os to position ourselves in the same place where our original folder resides
# and then we will do actual copying with shutil |to mess arond with file location

import os,shutil
os.chdir

from google.colab import drive
drive.mount('/content/gdrive')

os.getcwd()

#create a train folder
! mkdir ~/.train_sign

!rm -rf '/root/.train_sign'

! mkdir ~/.test_sign

!rm -rf '/root/.test_sign'

os.getcwd()

for key in categ:
  rec = os.path.join(folder_path,"Train/",key)
  !cp -av rec '/content/train_sign'

!cp -av '/content/sample_data/Traffic Sign dataset/Train/1' '/content/sample_data/Traffic Sign dataset/train_sign'
!cp -av '/content/sample_data/Traffic Sign dataset/Train/4' '/content/sample_data/Traffic Sign dataset/train_sign'
!cp -av '/content/sample_data/Traffic Sign dataset/Train/15' '/content/sample_data/Traffic Sign dataset/train_sign'
!cp -av '/content/sample_data/Traffic Sign dataset/Train/18' '/content/sample_data/Traffic Sign dataset/train_sign'
!cp -av '/content/sample_data/Traffic Sign dataset/Train/33' '/content/sample_data/Traffic Sign dataset/train_sign'
!cp -av '/content/sample_data/Traffic Sign dataset/Train/34' '/content/sample_data/Traffic Sign dataset/train_sign'



"""**Making Training and Validation Data**"""

import matplotlib.pyplot as plt
import seaborn as sns

import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import adam_experimental

from sklearn.metrics import classification_report,confusion_matrix

import tensorflow as tf

import cv2
import os

import numpy as np

pip install split-folders

from google.colab.patches import cv2_imshow
i = cv2.imread('/content/drive/MyDrive/traffic sign content/data split/train/15/00015_00000_00000.png')
plt.imshow(i)

"""https://www.youtube.com/watch?v=C6wbr1jJvVs&ab_channel=DigitalSreeni"""

#https://www.kaggle.com/questions-and-answers/102677
categ = {'1': '30 speed limit', '4':'70 speed limit', '15':'no entry', '18':'go slow', '33':'right hand curve', '34':'left hand curve' }
import splitfolders
splitfolders.ratio('/content/sample_data/Traffic Sign dataset/train_sign',output="/content/drive/MyDrive/traffic sign content/data split", seed=1337,ratio=(0.8,0.2))

categ = {'1': '30 speed limit', '4':'70 speed limit', '15':'no entry', '18':'go slow', '33':'right hand curve', '34':'left hand curve' }

test_file = pd.read_csv('/content/Test.csv')

test_file

len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/train/1')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/train/15')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/train/18')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/train/33')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/train/34')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/train/4'))

len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/val/1')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/val/15')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/val/18')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/val/33')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/val/34')) + len(os.listdir('/content/drive/MyDrive/traffic sign content/data split/val/4'))

"""**Putting input images into numpy array**"""

import cv2
data = []
labels = []

folder_path = '/content/train'

height = 25
width = 25
channels = 3
classes = 43
n_inputs = height * width * channels


for i in range(classes) :
  path = os.path.join(folder_path,str(i))
  print(path)
  class_path = os.listdir(path)
  for add in class_path:
    try:
      address = os.path.join(path,add)
      img = cv2.imread(address)
      image_from_array = Image.fromarray(img,'RGB')
      size_image = image_from_array.resize((height,width))
      data.append(np.array(size_image))
      labels.append(i)
    except AttributeError: print(" ")

Cells = np.array(data)
labels = np.array(labels)

#Randomize the order of the input images
s = np.arange(Cells.shape[0])
np.random.shuffle(s)
Cells=Cells[s]
labels=labels[s]

#Saving the Cells and labels file 
np.save("/content/gdrive/MyDrive/traffic sign content/Cells.npy",Cells)
np.save("/content/gdrive/MyDrive/traffic sign content/labels.npy",labels)
Cell1 = np.load("/content/gdrive/MyDrive/traffic sign content/Cells.npy")
labels1 = np.load("/content/gdrive/MyDrive/traffic sign content/labels.npy")

#Checking if 2 arrays are equal or not
if (Cell1 == Cells).all():
  print("t")

if (labels1 == labels).all():
  print("m")
print('T')

#Function to compare 2 files are equal or not
import filecmp.
filecmp. cmp('/content/gdrive/MyDrive/traffic sign content/Cells.npy', Cells)

type(labels), type(Cells), labels.shape, Cells.shape

#Spliting the images into train and validation sets
(X_train,X_val)=Cells[(int)(0.2*len(labels)):],Cells[:(int)(0.2*len(labels))]
X_train = X_train.astype('float32')/255 
X_val = X_val.astype('float32')/255
(y_train,y_val)=labels[(int)(0.2*len(labels)):],labels[:(int)(0.2*len(labels))]

#Using one hote encoding for the train and validation labels
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, 43)
y_val = to_categorical(y_val, 43)

#Definition of the CNN model

from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout

model = Sequential()

#1st CNN layer
model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))

#2nd CNN layer
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))

#3rd CNN layer
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))
model.add(Flatten())

#Fully Connected layer 1st layer
model.add(Dense(256, activation='relu'))
model.add(Dropout(rate=0.5))

#Fully Connected layer 2nd layer
model.add(Dense(43, activation='softmax'))

#Compilation of the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 20
history = model.fit(X_train, y_train, batch_size=32, epochs=epochs,validation_data=(X_val, y_val))

score = model.evaluate(X_train, y_train, verbose=0)
score
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))



# serialize model to JSON 
model_json = model.to_json()
with open("/content/gdrive/MyDrive/traffic sign content/model.json", "w") as json_file:
  json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("/content/gdrive/MyDrive/traffic sign content/model.h5")
print("Saved model to disk")

from keras.models import model_from_json
# load json and create model
json_file = open('/content/gdrive/MyDrive/traffic sign content/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("/content/gdrive/MyDrive/traffic sign content/model.h5")
print("Loaded model from disk")

loaded_model.compile( loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
score_save = loaded_model.evaluate(X_train, y_train,verbose=0)
print("%s: %.2f%%" % (loaded_model.metrics_names[1], score_save[1]*100))

import pickle
# save the model to disk
filename = '/content/gdrive/MyDrive/traffic sign content/model.sav'
pickle.dump(model, open(filename, 'wb'))

import pickle
# convert the history.history dict to a pandas DataFrame:     
hist_df = pd.DataFrame(history.history) 

# save to json:  
hist_json_file = '/content/gdrive/MyDrive/traffic sign content/history.json' 
with open(hist_json_file, mode='w') as f:
    hist_df.to_json(f)

# or save to csv: 
hist_csv_file = '/content/gdrive/MyDrive/traffic sign content/history.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

# load the model from disk
loaded_model_history = pickle.load(open(filename, 'rb'))

with open('/trainHistoryDict', 'wb') as file_pi:
        pickle.dump(history.history, file_pi)

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(loaded_model.loaded_model['accuracy'], label='training accuracy')
plt.plot(loaded_model.loaded_model['val_accuracy'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(loaded_model.loaded_model['loss'], label='training loss')
plt.plot(loaded_model.loaded_model['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['accuracy'], label='training accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()



"""**FITTING THE MODEL WITH TRAINING AND VALIDATION DATA : MODEL2**"""

no_of_classes = 6

model = Sequential()

#1st CNN layer
model.add(Conv2D(64,(3,3),padding = 'same', input_shape = (48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))

#Activation layer is persent at the end of the neuron and tells what data should be fired to teh nest layer
#Relu doesnpt triggers all the neuron at the same time

#2nd CNN layer
model.add(Conv2D(128,(5,5), padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))


#3rd CNN layer
model.add(Conv2D(512,(3,3), padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))

#4th CNN layer
model.add(Conv2D(512,(3,3),padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

#Fully connected 1st layer
model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

#Fully connected layer 2nd layer
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

model.add(Dense(no_of_classes, activation='softmax'))

opt = Adam(lr = 0.0001)
model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics =['accuracy'])
model.summary()

history = model.fit_generator(generator=train_set,
                              steps_per_epoch = train_set.n//train_set.batch_size,
                              epochs = epochs,
                              validation_data = val_set,
                              validation_steps = val_set.n//val_set.batch_size,
                              callbacks = callbacks_list)

import torch
mode = 'classifier.pt'
path = F"/content/output/model" 
torch.save(model.state_dict(), path)

#batch size is  a no. which depicts how much training example you model
#should take in one iteration
batch_size = 128
picture_size = 48

folder_path = "/content/drive/MyDrive/traffic sign content/data split/"

datagen_train = ImageDataGenerator()
datagen_val = ImageDataGenerator()

#/content/train_sign

train_set = datagen_train.flow_from_directory(folder_path+"train",target_size = 
                                              (picture_size,picture_size),
                                              color_mode = "grayscale",
                                              batch_size = batch_size,
                                              class_mode = 'categorical',
                                              shuffle = True)


val_set = datagen_val.flow_from_directory(folder_path+"val",target_size = 
                                              (picture_size,picture_size),
                                              color_mode = "grayscale",
                                              batch_size = batch_size,
                                              class_mode = 'categorical',
                                              shuffle = False)

from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

checkpoint = ModelCheckpoint("/content/drive/MyDrive/traffic sign content/data split/model.h5", monitor='val_acc', verbose = 1, save_best_only = True, mode = 'max')

early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3,
                               verbose=1, 
                               restore_best_weights=True)

reduce_learning = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2,
                                    patience=3, verbose=1, min_delta=0.0001)

callbacks_list = [early_stopping, checkpoint, reduce_learning]

epochs = 8



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization, Activation, MaxPooling2D
from keras.models import Model,Sequential
from tensorflow.keras.optimizers import Adam,SGD,RMSprop


import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import adam_experimental

from sklearn.metrics import classification_report,confusion_matrix

import tensorflow as tf

import cv2
import os

no_of_classes = 6

model = Sequential()

#1st CNN layer
model.add(Conv2D(64,(3,3),padding = 'same', input_shape = (48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))

#Activation layer is persent at the end of the neuron and tells what data should be fired to teh nest layer
#Relu doesnpt triggers all the neuron at the same time

#2nd CNN layer
model.add(Conv2D(128,(5,5), padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))


#3rd CNN layer
model.add(Conv2D(512,(3,3), padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))

#4th CNN layer
model.add(Conv2D(512,(3,3),padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

#Fully connected 1st layer
model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

#Fully connected layer 2nd layer
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

model.add(Dense(no_of_classes, activation='softmax'))

opt = Adam(lr = 0.0001)
model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics =['accuracy'])
model.summary()

#batch size is  a no. which depicts how much training example you model
#should take in one iteration
batch_size = 128
picture_size = 48

folder_path = "/content/drive/MyDrive/traffic sign content/data split/"

datagen_train = ImageDataGenerator()
datagen_val = ImageDataGenerator()

#/content/train_sign

train_set = datagen_train.flow_from_directory(folder_path+"train",target_size = 
                                              (picture_size,picture_size),
                                              color_mode = "grayscale",
                                              batch_size = batch_size,
                                              class_mode = 'categorical',
                                              shuffle = True)


val_set = datagen_val.flow_from_directory(folder_path+"val",target_size = 
                                              (picture_size,picture_size),
                                              color_mode = "grayscale",
                                              batch_size = batch_size,
                                              class_mode = 'categorical',
                                              shuffle = False)

from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

checkpoint = ModelCheckpoint("/content/drive/MyDrive/traffic sign content/data split/model.h5", monitor='val_acc', verbose = 1, save_best_only = True, mode = 'max')

early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3,
                               verbose=1, 
                               restore_best_weights=True)

reduce_learning = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2,
                                    patience=3, verbose=1, min_delta=0.0001)

callbacks_list = [early_stopping, checkpoint, reduce_learning]

epochs = 5

history = model.fit(generator=train_set,
                              steps_per_epoch = train_set.n//train_set.batch_size,
                              epochs = epochs,
                              validation_data = val_set,
                              validation_steps = val_set.n//val_set.batch_size,
                              callbacks = callbacks_list)

import pickle

with open('/content/drive/MyDrive/traffic sign content/data split/model1', 'wb') as f:
  pickle.dump(model,f)

with open('/content/drive/MyDrive/traffic sign content/data split/model1', 'rb') as f:
  save_model = pickle.load(f)

"""**Plotting Accuracy & Loss**"""

plt.style.use('dark_background')

plt.figure(figsize=(20,10))
plt.subplot(1,2,1)
plt.suptitle('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label = 'Validation Loss')
plt.legend(loc = 'upper right')

plt.subplot(1, 2, 2)
plt.ylabel('Accuracy', fontsize=16)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')
plt.legend(loc = 'lower right')
plt.show()

plt.style.use('dark_background')

plt.figure(figsize=(20,10))
plt.subplot(1,2,1)
plt.suptitle('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
plt.plot(save_model.save_model['loss'], label='Training Loss')
plt.plot(save_model.save_model['val_loss'], label = 'Validation Loss')
plt.legend(loc = 'upper right')

plt.subplot(1, 2, 2)
plt.ylabel('Accuracy', fontsize=16)
plt.plot(save_model.save_model['accuracy'], label='Training Accuracy')
plt.plot(save_model.save_model['val_accuracy'], label = 'Validation Accuracy')
plt.legend(loc = 'lower right')
plt.show()



"""**Testing our model with test dataset**"""

import numpy as np
import pandas as pd
import matplotlib as plt

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

from keras.models import model_from_json
# load json and create model
json_file = open('/content/gdrive/MyDrive/traffic sign content/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("/content/gdrive/MyDrive/traffic sign content/model.h5")
print("Loaded model from disk")

from PIL import Image
from sklearn.metrics import accuracy_score
import pandas as pd
y_test = pd.read_csv('/content/Test.csv')

labels = y_test['ClassId'].values
imgs = y_test['Path'].values

data1=[]

for img in imgs:
  image = Image.open(img)
  image = image.resize((25,25))
  data1.append(np.array(image))

X_test = np.array(data1)

len(data1)

predict_x=loaded_model.predict(X_test) 
pred=np.argmax(predict_x,axis=1)
#pred = loaded_model.predict_classes(X_test)

#Accuracy with the test data
from sklearn.metrics import accuracy_score
accuracy_score(labels,pred)



"""https://towardsdatascience.com/how-to-calculate-the-number-of-parameters-in-keras-models-710683dae0ca"""

loaded_model.summary()

